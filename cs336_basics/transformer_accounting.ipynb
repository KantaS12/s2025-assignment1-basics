{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4c782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 50257 # Vocab Size\n",
    "T = 1024 # Context Length\n",
    "L = 48 # Num Layers\n",
    "d = 1600 # Model Dim\n",
    "H = 25 # Num Heads\n",
    "d_ff = 6400 # Feedforward Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85311fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 1.56 Billion\n",
      "Memory Load: 5.80 GB\n"
     ]
    }
   ],
   "source": [
    "# (a) Parameter Count\n",
    "\n",
    "# Embedding (Token + Positional)\n",
    "token = V * d\n",
    "positional = T * d\n",
    "\n",
    "embedding_total = token + positional\n",
    "\n",
    "# Per Layer (Attention + MLP)\n",
    "\n",
    "# Attention (4 Matrices of shape (d, d) -> Q, K, V, Output)\n",
    "attention = 4 * (d * d)\n",
    "\n",
    "# MLP (2 Matrices of shape (d, d_ff) and (d_ff, d))\n",
    "mlp = 2 * (d * d_ff)\n",
    "\n",
    "# Layer Normalization (2 per layer)\n",
    "layer_norm = 2 * (2 * d)  # 2 parameters (scale & shift)\n",
    "\n",
    "param_per_layer = attention + mlp + layer_norm\n",
    "\n",
    "total = embedding_total + (param_per_layer * L)\n",
    "\n",
    "print(f\"Total Parameters: {total / 1e9:.2f} Billion\")\n",
    "\n",
    "# Memory Load\n",
    "total_bytes = total * 4  # 4 bytes per parameter (float32)\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Memory Load: {total_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71401ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPs for Forward Pass: 3.02 TFLOPs\n"
     ]
    }
   ],
   "source": [
    "# (b) Matrix Multiplication to complete a forward pass\n",
    "# How many FLOPs matrix multiplications are needed\n",
    "\n",
    "num_flops = 2 * T * L * (\n",
    "    (4 * d * d) +  # Attention\n",
    "    (2 * d * d_ff)  # MLP\n",
    ")\n",
    "\n",
    "print(f\"Total FLOPs for Forward Pass: {num_flops / 1e12:.2f} TFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b7f493c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flops in Attention: 33.33%\n",
      "Flops in MLP: 66.67%\n"
     ]
    }
   ],
   "source": [
    "# (c) Which parts of the model require the most FLOPs?\n",
    "\n",
    "print(f\"Flops in Attention: { (2 * T * L * 4 * d * d) / num_flops * 100:.2f}%\")\n",
    "print(f\"Flops in MLP: { (2 * T * L * 2 * d * d_ff) / num_flops * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ace5431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 0.19 Billion\n",
      "Memory Load: 0.69 GB\n",
      "Total FLOPs for Forward Pass: 0.30 TFLOPs\n",
      "Flops in Attention: 19.35%\n",
      "Flops in MLP: 80.65%\n"
     ]
    }
   ],
   "source": [
    "# (d) Analysis 1 (GPT-2 Small)\n",
    "V = 50257 # Vocab Size\n",
    "T = 1024 # Context Length\n",
    "L = 12 # Num Layers\n",
    "d = 768 # Model Dim\n",
    "H = 12 # Num Heads\n",
    "d_ff = 6400 # Feedforward Dimension\n",
    "\n",
    "# (a) Parameter Count\n",
    "\n",
    "# Embedding (Token + Positional)\n",
    "token = V * d\n",
    "positional = T * d\n",
    "\n",
    "embedding_total = token + positional\n",
    "\n",
    "# Per Layer (Attention + MLP)\n",
    "\n",
    "# Attention (4 Matrices of shape (d, d) -> Q, K, V, Output)\n",
    "attention = 4 * (d * d)\n",
    "\n",
    "# MLP (2 Matrices of shape (d, d_ff) and (d_ff, d))\n",
    "mlp = 2 * (d * d_ff)\n",
    "\n",
    "# Layer Normalization (2 per layer)\n",
    "layer_norm = 2 * (2 * d)  # 2 parameters (scale & shift)\n",
    "\n",
    "param_per_layer = attention + mlp + layer_norm\n",
    "\n",
    "total = embedding_total + (param_per_layer * L)\n",
    "\n",
    "print(f\"Total Parameters: {total / 1e9:.2f} Billion\")\n",
    "\n",
    "# Memory Load\n",
    "total_bytes = total * 4  # 4 bytes per parameter (float32)\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Memory Load: {total_gb:.2f} GB\")\n",
    "\n",
    "# (b) Matrix Multiplication to complete a forward pass\n",
    "# How many FLOPs matrix multiplications are needed\n",
    "\n",
    "num_flops = 2 * T * L * (\n",
    "    (4 * d * d) +  # Attention\n",
    "    (2 * d * d_ff)  # MLP\n",
    ")\n",
    "\n",
    "print(f\"Total FLOPs for Forward Pass: {num_flops / 1e12:.2f} TFLOPs\")\n",
    "\n",
    "# (c) Which parts of the model require the most FLOPs?\n",
    "\n",
    "print(f\"Flops in Attention: { (2 * T * L * 4 * d * d) / num_flops * 100:.2f}%\")\n",
    "print(f\"Flops in MLP: { (2 * T * L * 2 * d * d_ff) / num_flops * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a586f25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 0.47 Billion\n",
      "Memory Load: 1.74 GB\n",
      "Total FLOPs for Forward Pass: 0.85 TFLOPs\n",
      "Flops in Attention: 24.24%\n",
      "Flops in MLP: 75.76%\n"
     ]
    }
   ],
   "source": [
    "# Analysis 2 (GPT-2 Medium)\n",
    "V = 50257 # Vocab Size\n",
    "T = 1024 # Context Length\n",
    "L = 24 # Num Layers\n",
    "d = 1024 # Model Dim\n",
    "H = 16 # Num Heads\n",
    "d_ff = 6400 # Feedforward Dimension\n",
    "\n",
    "# (a) Parameter Count\n",
    "\n",
    "# Embedding (Token + Positional)\n",
    "token = V * d\n",
    "positional = T * d\n",
    "\n",
    "embedding_total = token + positional\n",
    "\n",
    "# Per Layer (Attention + MLP)\n",
    "\n",
    "# Attention (4 Matrices of shape (d, d) -> Q, K, V, Output)\n",
    "attention = 4 * (d * d)\n",
    "\n",
    "# MLP (2 Matrices of shape (d, d_ff) and (d_ff, d))\n",
    "mlp = 2 * (d * d_ff)\n",
    "\n",
    "# Layer Normalization (2 per layer)\n",
    "layer_norm = 2 * (2 * d)  # 2 parameters (scale & shift)\n",
    "\n",
    "param_per_layer = attention + mlp + layer_norm\n",
    "\n",
    "total = embedding_total + (param_per_layer * L)\n",
    "\n",
    "print(f\"Total Parameters: {total / 1e9:.2f} Billion\")\n",
    "\n",
    "# Memory Load\n",
    "total_bytes = total * 4  # 4 bytes per parameter (float32)\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Memory Load: {total_gb:.2f} GB\")\n",
    "\n",
    "\n",
    "num_flops = 2 * T * L * (\n",
    "    (4 * d * d) +  # Attention\n",
    "    (2 * d * d_ff)  # MLP\n",
    ")\n",
    "\n",
    "print(f\"Total FLOPs for Forward Pass: {num_flops / 1e12:.2f} TFLOPs\")\n",
    "\n",
    "print(f\"Flops in Attention: { (2 * T * L * 4 * d * d) / num_flops * 100:.2f}%\")\n",
    "print(f\"Flops in MLP: { (2 * T * L * 2 * d * d_ff) / num_flops * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab9b382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 0.89 Billion\n",
      "Memory Load: 3.32 GB\n",
      "Total FLOPs for Forward Pass: 1.69 TFLOPs\n",
      "Flops in Attention: 28.57%\n",
      "Flops in MLP: 71.43%\n"
     ]
    }
   ],
   "source": [
    "# Analysis 2 (GPT-2 Large)\n",
    "V = 50257 # Vocab Size\n",
    "T = 1024 # Context Length\n",
    "L = 36 # Num Layers\n",
    "d = 1280 # Model Dim\n",
    "H = 20 # Num Heads\n",
    "d_ff = 6400 # Feedforward Dimension\n",
    "\n",
    "# (a) Parameter Count\n",
    "\n",
    "# Embedding (Token + Positional)\n",
    "token = V * d\n",
    "positional = T * d\n",
    "\n",
    "embedding_total = token + positional\n",
    "\n",
    "# Per Layer (Attention + MLP)\n",
    "\n",
    "# Attention (4 Matrices of shape (d, d) -> Q, K, V, Output)\n",
    "attention = 4 * (d * d)\n",
    "\n",
    "# MLP (2 Matrices of shape (d, d_ff) and (d_ff, d))\n",
    "mlp = 2 * (d * d_ff)\n",
    "\n",
    "# Layer Normalization (2 per layer)\n",
    "layer_norm = 2 * (2 * d)  # 2 parameters (scale & shift)\n",
    "\n",
    "param_per_layer = attention + mlp + layer_norm\n",
    "\n",
    "total = embedding_total + (param_per_layer * L)\n",
    "\n",
    "print(f\"Total Parameters: {total / 1e9:.2f} Billion\")\n",
    "\n",
    "# Memory Load\n",
    "total_bytes = total * 4  # 4 bytes per parameter (float32)\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Memory Load: {total_gb:.2f} GB\")\n",
    "\n",
    "\n",
    "num_flops = 2 * T * L * (\n",
    "    (4 * d * d) +  # Attention\n",
    "    (2 * d * d_ff)  # MLP\n",
    ")\n",
    "\n",
    "print(f\"Total FLOPs for Forward Pass: {num_flops / 1e12:.2f} TFLOPs\")\n",
    "\n",
    "\n",
    "print(f\"Flops in Attention: { (2 * T * L * 4 * d * d) / num_flops * 100:.2f}%\")\n",
    "print(f\"Flops in MLP: { (2 * T * L * 2 * d * d_ff) / num_flops * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1c9528c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPs: 133.42 TFLOPs\n",
      "Relative Contribution of Components:\n",
      "  - Linear Projections (Dense Layers): 36.2%\n",
      "  - Attention Mechanism (Quadratic):   61.8%\n"
     ]
    }
   ],
   "source": [
    "# (e) Increase context length to 16,384. How does FlOPs for one forward pass change? \n",
    "# How does relative contribution of FLOPs of the model components change?\n",
    "\n",
    "V = 50257 # Vocab Size\n",
    "T = 16384 # Context Length\n",
    "L = 48 # Num Layers\n",
    "d = 1600 # Model Dim\n",
    "H = 25 # Num Heads\n",
    "d_ff = 6400 # Feedforward Dimension\n",
    "\n",
    "flops_linear_per_layer = 2 * T * ( (4*d*d) + (2*d*d_ff) )\n",
    "\n",
    "flops_quadratic_per_layer = 2 * (2 * T * T * d)\n",
    "\n",
    "flops_logits = 2 * T * d * V\n",
    "\n",
    "total_flops = L * (flops_linear_per_layer + flops_quadratic_per_layer) + flops_logits\n",
    "\n",
    "prop_linear = (L * flops_linear_per_layer) / total_flops\n",
    "prop_quadratic = (L * flops_quadratic_per_layer) / total_flops\n",
    "\n",
    "print(f\"Total FLOPs: {total_flops / 1e12:.2f} TFLOPs\")\n",
    "print(f\"Relative Contribution of Components:\")\n",
    "print(f\"  - Linear Projections (Dense Layers): {prop_linear:.1%}\")\n",
    "print(f\"  - Attention Mechanism (Quadratic):   {prop_quadratic:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE405",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
