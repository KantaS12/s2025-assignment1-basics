{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e9b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) How much peak memory does running AdamW require?\n",
    "# Answer based on the memory usage of the parameters, gradients, and optimizer states.\n",
    "# Answer in term of batch_size and model hyper parameters (vocab_size, context_length, num_layers, d_model, num_heads). \n",
    "# Assume d_ff = 4 * d_model\n",
    "# Give algebraic expressions only, no numbers.\n",
    "peak_memory = (batch_size * context_length * (d_model + 4 * d_model) * num_layers * num_heads) * 3 + (vocab_size * d_model) + (num_layers * d_model * d_model * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f1a6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Batch Size: 12284768211.92053\n"
     ]
    }
   ],
   "source": [
    "V = 50257 # Vocab Size\n",
    "T = 1024 # Context Length\n",
    "L = 48 # Num Layers\n",
    "d = 1600 # Model Dim\n",
    "H = 25 # Num Heads\n",
    "d_ff = 6400 # Feedforward Dimension\n",
    "# (b) Instantiate GPT-2 XL model to get an expression that only depends on the batch_size. \n",
    "# What is the maximum. batch size you can use and still fit within 80GB memory?\n",
    "# Total Parameters: 1.56 Billion\n",
    "# Memory Load: 5.80 GB\n",
    "# Total FLOPs for Forward Pass: 3.02 TFLOPs\n",
    "# Flops in Attention: 33.33%\n",
    "# Flops in MLP: 66.67%\n",
    "# Output an expression that looks like:\n",
    "# a * batch_size + b for some numerical values a,b and a number representing the max batch size\n",
    "\n",
    "a = 0.00000000302 * 2 # 2 for forward and backward\n",
    "b = 5.80\n",
    "max_memory = 80 # GB\n",
    "max_batch_size = (max_memory - b) / a\n",
    "print(f\"Max Batch Size: {max_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ece7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) How many FLOPs does running one step of AdamW. take?\n",
    "# Answer in algebraic expression & explanation\n",
    "flops_per_param_adamw = 8 # 2 for gradient computation, 6 for AdamW update\n",
    "total_params = 1.56e9 # 1.56 Billion\n",
    "total_flops_adamw = flops_per_param_adamw * total_params\n",
    "print(f\"Total FLOPs for AdamW step: {total_flops_adamw / 1e12} TFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (d) \n",
    "# Effective Throughput = Peak FLOP/s * MFU\n",
    "# Given Peak FLOP/s = 19.5 teraFLOP/s and MFU = 0.5\n",
    "# 9.75 teraFlOP/s\n",
    "\n",
    "# Forward Pass\n",
    "# FLOPs per token 6N, parameters 1.5B \n",
    "# Forward Pass = 6 * 1.5B = 9 * 10^12 FLOPs/token\n",
    "\n",
    "# Forward + Backward = 9 × 10¹² + 2(9 × 10¹²) = 27 × 10¹² FLOPs/token\n",
    "\n",
    "# Total Flops = 400,000 steps * 27 × 10¹² FLOPs/token = 1.08 × 10¹⁹ FLOPs\n",
    "\n",
    "# Time = Total Flops / Effective Throughput\n",
    "#      = 1.08 × 10¹⁹ FLOPs / 9.75 × 10¹² FLOPs/s\n",
    "#      = 1.108 × 10⁶ seconds\n",
    "#      = 12.83 days"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE405",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
