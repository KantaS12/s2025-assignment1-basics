{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e9b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) How much peak memory does running AdamW require?\n",
    "# Answer based on the memory usage of the parameters, gradients, and optimizer states.\n",
    "# Answer in term of batch_size and model hyper parameters (vocab_size, context_length, num_layers, d_model, num_heads). \n",
    "# Assume d_ff = 4 * d_model\n",
    "\n",
    "# Parameters\n",
    "# Embeddings: vocab_size * d_model\n",
    "# Attention Weights: 4 * d_model^2 (Q, K, V, O) per layer\n",
    "# FFN Weights: 2 * (d_model * 4 * d_model) = 8 * d_model^2\n",
    "# Total per Layer: 12 * d_model^2\n",
    "# P = 12  * L * D^2 + V * D\n",
    "\n",
    "# Gradients\n",
    "# G = P\n",
    "\n",
    "# Optimizer States \n",
    "# O = 2 * P\n",
    "\n",
    "# Activations\n",
    "# Linear Layers (Q, K, V, FFN expansion, FFN projection): 14 * batch_size * context_length * d_model\n",
    "# Attention Matrix (B, H, T, T): 2 * B * H * T^2\n",
    "\n",
    "# Logits \n",
    "# L = batch_size * context_length * vocab_size\n",
    "\n",
    "# A = L * (14 * B * T * D + 2 * B * H * T^2) + B * T * V\n",
    "\n",
    "# Total Bytes = 4 * [4(12LD^2 + VD) + L(14BTD + 2BHT^2) + BTV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f1a6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Batch Size: 12284768211.92053\n"
     ]
    }
   ],
   "source": [
    "V = 50257  # Vocab Size\n",
    "T = 1024   # Context Length\n",
    "L = 48     # Num Layers\n",
    "d = 1600   # Model Dim\n",
    "H = 25     # Num Heads\n",
    "d_ff = 4 * d  # Feedforward Dimension\n",
    "\n",
    "# (b) Instantiate GPT-2 XL into the formula from (a)\n",
    "# From part (a):\n",
    "#   N = 2*V*d + L*(12*d^2 + 2*d) + d\n",
    "#   M_peak = 16*N + 4*[L*(19*B*T*d + 2*B*H*T^2) + B*T*d + B*T*V]\n",
    "#\n",
    "# Separate into constant (independent of B) and B-dependent terms:\n",
    "#   M_peak = 16*N + 4*B*T*[L*(19*d + 2*H*T) + d + V]\n",
    "#          = (constant term) + (per-batch term) * B\n",
    "\n",
    "N = 2 * V * d + L * (12 * d**2 + 2 * d) + d\n",
    "constant_bytes = 16 * N  # params + grads + optimizer state\n",
    "constant_gb = constant_bytes / (1024**3)\n",
    "\n",
    "per_batch_bytes = 4 * T * (L * (19 * d + 2 * H * T**2 // T) + d + V)\n",
    "per_batch_bytes = 4 * T * (L * (19 * d + 2 * H * T) + d + V)\n",
    "per_batch_gb = per_batch_bytes / (1024**3)\n",
    "\n",
    "print(f\"Total Parameters N = {N:,}\")\n",
    "print(f\"N ≈ {N/1e9:.2f} billion\")\n",
    "print(f\"Constant term (16N):  {constant_gb:.2f} GB\")\n",
    "print(f\"Per-batch coefficient: {per_batch_gb:.4f} GB per sample\")\n",
    "print(f\"M_peak (GB) = {per_batch_gb:.4f} * B + {constant_gb:.2f}\")\n",
    "\n",
    "# Max batch size for 80 GB\n",
    "max_memory_gb = 80\n",
    "max_B = int((max_memory_gb - constant_gb) / per_batch_gb)\n",
    "print(f\"Max batch size for {max_memory_gb} GB: B = {max_B}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ece7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) How many FLOPs does running one step of AdamW. take?\n",
    "# Answer in algebraic expression & explanation\n",
    "flops_per_param_adamw = 8 # 2 for gradient computation, 6 for AdamW update\n",
    "total_params = 1.56e9 # 1.56 Billion\n",
    "total_flops_adamw = flops_per_param_adamw * total_params\n",
    "print(f\"Total FLOPs for AdamW step: {total_flops_adamw / 1e12} TFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (d) \n",
    "# Effective Throughput = Peak FLOP/s * MFU\n",
    "# Given Peak FLOP/s = 19.5 teraFLOP/s and MFU = 0.5\n",
    "# 9.75 teraFlOP/s\n",
    "\n",
    "# Forward Pass\n",
    "# FLOPs per token 6N, parameters 1.5B \n",
    "# Forward Pass = 6 * 1.5B = 9 * 10^12 FLOPs/token\n",
    "\n",
    "# Forward + Backward = 9 × 10¹² + 2(9 × 10¹²) = 27 × 10¹² FLOPs/token\n",
    "\n",
    "# Total Flops = 400,000 steps * 27 × 10¹² FLOPs/token = 1.08 × 10¹⁹ FLOPs\n",
    "\n",
    "# Time = Total Flops / Effective Throughput\n",
    "#      = 1.08 × 10¹⁹ FLOPs / 9.75 × 10¹² FLOPs/s\n",
    "#      = 1.108 × 10⁶ seconds\n",
    "#      = 12.83 days"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE405",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
